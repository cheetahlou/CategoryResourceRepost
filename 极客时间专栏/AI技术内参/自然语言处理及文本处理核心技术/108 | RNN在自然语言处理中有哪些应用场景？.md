<audio id="audio" title="108 | RNN在自然语言处理中有哪些应用场景？" controls="" preload="none"><source id="mp3" src="https://static001.geekbang.org/resource/audio/c6/fa/c6669ffb09c001238ed6891719b892fa.mp3"></audio>

周一我们进一步展开了RNN这个基本框架，讨论了几个流行的RNN模型实现，从最简单的RNN模型到为什么需要“门机制”，再到流行的LSTM和GRU框架的核心思想。

今天，我们就来看一看RNN究竟在自然语言处理的哪些任务和场景中有所应用。

## 简单分类场景

我们首先来聊一种**简单的分类场景**。在这种场景下，RNN输入一个序列的文字，然后根据所有这些文字，做一个决策，或者叫作输出一个符号。这类应用是文本挖掘和分析中最基本的一个场景。

在绝大多数的“简单分类”任务中，传统的文字表达，例如“**词包**”（Bag of Word）或者“**N元语法**”（Ngram），经常都能有不错的表现。也就是说，在很多这类任务中，文字的顺序其实并不是很重要，或者说词序并没有携带更多的语义信息。

然而，实践者们发现，在一些场景中，如果利用RNN来对文字序列进行建模，会获得额外的效果提升。比如有一类任务叫作“**句子级别的情感分类**”（Sentence-Level Sentiment Classification），这类任务常常出现在分析商品的评论文本（Review）这个场景。这时候，我们需要对每一个句子输出至少两种感情色彩的判断，褒义或者贬义，正面或者负面。比如，我们在分析电影评价的时候，就希望知道用户在某一个句子中是否表达了对电影“喜爱”或者“不喜爱”的情绪。

面对这样句子级别的情感分析，一种比较通行的利用RNN建模的方式是：**把每一个单词作为一个输入单元，然后把一个句子当作一个序列输入到一个RNN中去，RNN来维持一个隐含的状态**。

对于这类应用，不是每一个隐含状态都有一个输出，而是在句子结束的时候，利用最后的隐含状态来产生输出。对于这类任务而言，输出的状态就是一个二元判断，那么我们需要利用最后的隐含状态来实现这个目的。一般来说，在深度模型的架构中，这个步骤是利用最后的隐含状态，然后经过多层感知网络，最后进行一个二元或者多元的分类。这其实是一个标准的分类问题的构建。

在有的应用中，研究者们发现可以**利用两个RNN建立起来的链条**，从而能够更进一步地提升最后的分类效果。在我们刚才描述的建模步骤里，RNN把一个句子从头到尾按照正常顺序进行了输入并归纳。另外一种建模方式是利用RNN去建模**句子的逆序**，也就是把整个句子倒过来，学习到一个逆序的隐含状态。接下来，我们把顺序的最后隐含状态和逆序的最后隐含状态串联起来，成为最终放入分类器需要学习的特性。这种架构有时候被称作“**双向模型**”。

当我们从句子这个层级到文档这个层级时，比如希望对文档进行情感分类，仅仅利用我们刚才讲的RNN的结构就会显得有点“捉襟见肘”了。一个重要的阻碍就是RNN很难针对特别长的序列直接建模。这个时候，就需要我们把整个文档**拆分**成比较小的单元，然后针对小的单元利用RNN进行建模，再把这些小单元的RNN结果当作新的输入串联起来。

在实际拆分的时候，我们可以把文章分成一个一个的句子，然后每个句子可以用刚才我们在句子层级的建模方式进行建模；在句子的层级下，还可能再把句子拆分成比如短语这样的单元。这种把一个比较大的文档进行拆分，并且通过RNN对不同级别的数据进行建模的形式就叫作**“层次式”（Hierarchical）RNN建模**。

## 特性提取器

在更多的场景中，RNN其实已经扮演了**文本信息特性提取器**的角色，特别是在很多监督学习任务中，隐含状态常常被用来当作特性处理。尤其要说明的是，如果你的任务对文字的顺序有一定要求，RNN往往就能成为这方面任务的利器，我们这里举几个例子。

首先可以想到的一个任务就是，在自然语言处理中很常见的“**词类标注**”（Part-Of-Speech Tagging），或者简称**POS标注**。简单来说，POS标注就是针对某一个输入句子，把句子里的词性进行分析和标注，让大家知道哪些是动词，哪些是名词，哪些是形容词等等。我们可以很容易地想到，在这样的标注任务中，一个词到底是名词还是动词，在很多的语言场景中，是需要对整个句子的语境进行分析的，也就是说，整个句子的顺序和词序是有意义的。

针对POS标注这类任务，一种已经尝试过的架构，就是利用我们刚才介绍过的**双向RNN**来对句子进行建模。双向RNN的好处是，我们可以构建的隐含信息是包含上下文的，这样就更加有助于我们来分析每个词的词性。

和句子分类的任务类似的是，利用双向RNN对句子进行扫描之后，我们依然需要建立**分类器**，对每一个位置上的词语进行分类。这个时候，依然是同样的思路，我们把当前的隐含状态当作是特性，利用多层感知网络，构建多类分类器，从而对当前位置的词性进行决策。

除了POS标注这样的任务以外，针对**普通的文档分类**，RNN也有一定的效果。这里我们所说的文档分类，一般是指类似把文档分为“艺术”、“体育”或“时政”等主题类别。人们从实践中发现，在这样的通用文档分类任务中，RNN和另外一类重要的深度模型，“**卷积神经网络**”（CNN）结合起来使用效果最好。我们这里不展开对CNN的原理进行讲解，只是从大的逻辑上为你讲一下这种分类方法的核心思路。

在计算机视觉中，通常认为CNN可以抓住图像的“位置”特征。也就是说，CNN非常善于挖掘一个二维数据结构中局部的很多变化特征，从而能够有效形成对这些数据点的总结。那么，如果我们把文档的文字排列也看作是某种情况下的一种图案，CNN就可以发挥其作用来对文字的上下文进行信息提取。然后当CNN对文字的局部信息进行提取之后，我们再把这些局部信息当作输入放入RNN中，这样就能更好地利用RNN去对文章的高维度的特征进行建模。

## 总结

今天我为你介绍了文本序列建模利器RNN的几个应用场景。

一起来回顾下要点：第一，我们讲了用RNN对句子层级进行分类任务的处理；第二，我们聊了如何把RNN当作普遍使用的特性提取器来进行分类任务的训练，特别是POS标签任务。

最后，给你留一个思考题，利用RNN提取的信息能否完整捕捉文档里的内容，这一点我们怎么来判断呢？

欢迎你给我留言，和我一起讨论。


