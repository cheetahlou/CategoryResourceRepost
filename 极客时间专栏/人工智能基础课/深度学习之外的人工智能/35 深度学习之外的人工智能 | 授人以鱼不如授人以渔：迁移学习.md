<audio id="audio" title="35 深度学习之外的人工智能 | 授人以鱼不如授人以渔：迁移学习" controls="" preload="none"><source id="mp3" src="https://static001.geekbang.org/resource/audio/b1/f0/b1be07f2f129ff4adc23baad408845f0.mp3"></audio>

无论是小学还是大学，在教学中都会强调的一个问题就是“举一反三”，将学到的规律灵活地运用到其他相似的场景下。而要想让人工智能学会举一反三，用到的就是**迁移学习技术**。

**迁移学习（transfer learning）是运用已学习的知识来求解不同但相关领域问题的新的机器学习方法，目的是让机器“学会学习”**。当训练数据和测试数据来自不同的特征空间或遵循不同的概率分布时，如果能够将从训练数据上习得的知识迁移到测试数据上，就可以回避掉复杂的数据标签工作，进而提升学习性能。迁移学习就是解决这个问题的学习框架，它能够对仅有少量甚至没有标签样本进行学习，从而解决目标问题。

许多机器学习和数据挖掘算法都建立在两个主要假设之上：第一，训练样本和测试数据必须处于相同的特征空间并具有相同的分布；第二，有足够的高质量训练样本作为学习资源。

遗憾的是，这两个假设在大多数实际应用中难以成立。一方面，训练数据和测试数据在时间上的差异可能导致分布规律的变化；另一方面，对大量数据进行标注不仅费时费力，还容易受到知识产权问题的影响。一旦没有数据，再好的深度学习方法都是无源之水，无本之木，难以发挥作用。

迁移学习的出现给解决这些问题带来了一丝曙光。其实说到底，**迁移学习可以看作是提升学习算法泛化性能的一种思路**。现实世界并非标准化和结构化的数据集，而是杂乱无章的万花筒，包含着大量从未在训练集中出现过的全新场景，这足以让许多在训练集上无往不胜的人工智能变成真实世界中的“人工智障”。**迁移学习有助于算法处理全新场景下的问题，利用一般化的规律去应对富于变化的环境**。

在迁移学习中，已有的知识（包括样本数据集及其分布）叫做**源域**，要学习的新知识叫做**目标域**。同样，待解决的任务也可以分为**源任务**和**目标任务**。根据源域/目标域和源任务/目标任务的关系，迁移学习问题可以划分为以下三类：

- **归纳迁移学习**（inductive transfer learning）：源域与目标域可能相同也可能不同，目标任务与源任务不同；
- **直推式迁移学习**（transductive transfer learning）：源域与目标域不同，目标任务与源任务相同；
- **无监督迁移学习**（unsupervised transfer learning）：目标任务与源任务不同，且源域数据和目标域数据都没有标签。

**在从源域到目标域的迁移过程中，迁移学习要解决的三个问题是：迁移什么、能不能迁移、如何迁移**。

“迁移什么”明确的是迁移学习的作用对象。显然，迁移的对象应该是普适性的知识，而不是特定领域的知识。用来背古诗词的技巧不一定适用于背化学方程式，这时需要迁移的就是整体意义上的记忆方法。

在确定了迁移什么之后，接下来就要解决能不能迁移的问题，只有当源域和目标域具有较高的相关性时，迁移才会有助于学习。如果不管三七二十一，把知识生搬硬套到风马牛不相及的问题上，反而会起到负作用。这就像骑自行车的技巧可以用在骑摩托车上，因为它们都有两个轮子，可如果用骑自行车的方法骑三轮车的话，只怕是要翻车了。

解决了上面两个问题之后，面对的就是迁移学习中的核心问题——**如何迁移**。迁移学习的具体方法包括以下四种：

- **基于样本的迁移学习**（instance transfer）；
- **基于特征的迁移学习**（feature representation transfer）；
- **基于模型的迁移学习**（parameter transfer）；
- **基于关系的迁移学习**（relational knowledge transfer）。

**基于样本的迁移学习是对已有样本的重用过程，它通过调整源域中原始样本的权重系数，使之和目标域匹配，进而应用在目标域中**。这种方法通过度量训练样本和测试样本之间的相似度来重新分配源域数据的采样权重，相似度越大的样本对目标任务的训练越有利，其权重也会得到强化，相似度小的样本权重则被削弱。

**在归纳迁移学习中，TrAdaBoost是典型的基于样本的方法**。它假定两个域中的数据具有不同的分布，但使用完全相同的特征和标签。由于源域和目标域之间的分布差异，不同的源域数据对目标任务的学习既可能有用也可能有害。TrAdaBoost通过多次迭代的方式调整源域数据的权重，以鼓励“好”数据和抑制“坏”数据。

基于样本的方法也可以用于直推式迁移学习之中。这种情况下，学习的出发点是使经验风险最小化，使用的方法则是基于重要性采样的方法。重要性采样的作用在于将关于未知的目标域数据的期望转化为关于目标域数据和已知的源域数据之间关系的期望，对两组数据关系的估计则可以通过核均值匹配或者使KL散度最小化等方法完成。

**基于特征的迁移学习是特征表示的重建过程，它通过特征变换使得源域数据与目标域数据处在同一个特征空间之上，再在这个公共空间上进行学习**。这种方法适用于所有的迁移学习任务。特征迁移的基本思想是学习在相关任务中共享的一组低维特征表示，对特征的降维采用的也是特征映射和特征选择两种方式（降维学习的两种方式）。

基于特征选择的迁移学习是识别出源领域与目标领域中共有的特征表示，再基于这些共有特征实现知识迁移。由于与样本类别高度相关的特征应该在模型中具备更高的权重，因此在找出源域和目标域的共同特征后，需要从目标域数据中选择特有的特征来对共同特征训练出的通用分类器进行精化，从而得到适合于目标域数据的分类器。

基于特征映射的迁移学习是把每个领域的数据从原始高维特征空间映射到新的低维特征空间，使源域数据和目标域数据在新的低维空间上具有相同的分布，这样就可以利用低维空间表示的有标签的源域数据来训练目标域上的分类器。特征映射和特征选择的区别在于映射得到的特征不属于原始的特征的子集，而是全新的特征。

**基于模型的迁移学习是已有模型的转移过程，它假设源任务和目标任务共享一些参数或者一些先验分布，将在训练数据上训练好的成熟模型应用到目标域上解决问题**。应用在归纳迁移学习中的大多数模型方法都来自于多任务学习，但多任务学习试图同时提升源任务和目标任务的学习效果，而迁移学习只是利用源域数据来提高目标域的性能。在迁移学习中，通过给目标域的损失函数分配更大的权重，可以确保在目标域中获得更好的性能。

**基于关系的迁移学习是问题结构的复制过程，如果源域和目标域之间共享了某种相似关系，那就可以将源域上的逻辑关系网络应用到目标域上**。与其他三种方法不同，关系学习方法处理的是关系域中的迁移学习问题，其中的数据在概率上不需要满足独立同分布的条件，但是一定要能够用类似的关系来表示，最典型的实例就是物理网络的数据和社交网络的数据。在这种方法中，数理逻辑是常用的工具。

除了以上的具体方法之外，迁移学习还需要解决一系列理论问题。既然迁移学习的目的是让机器学会学习，向人类学习的机制取经就是最简单的途径。人类学习的一个重要特点就是能够从纷繁复杂的现象中抽象出共性的问题，实现实例和规律的剥离。如果机器可以学会这种思维，把问题的内容和结构区分开来，其举一反三的能力就会产生质的飞跃。

另一方面，从迁移学习很容易进一步得到“**元学习**”（meta-learning）的概念。元学习的目标是通过对元数据的自动化学习来提升学习算法的灵活性，是对学习方法的学习。每个特定的学习算法都建立在一系列有关数据的假设基础上，这叫作**归纳偏差**。只有当归纳偏差和学习问题相匹配时，算法才能获得良好的效果。这不仅给机器学习和数据挖掘算法添加了应用的约束，给它们的实用性造成了相当大的限制。

正因如此，**学习的灵活性对通用人工智能至关重要**。通过使用学习问题的属性、算法属性、派生模式等不同类型的元数据，元学习可以对不同的学习算法进行学习，选择，更改或组合，以有效地解决给定的学习问题。其实遗传进化就是对基因编码的学习过程，这个过程在大脑中执行，这意味着我们每个人其实都是元学习的践行者。但是元学习到底能不能推广到机器学习之中，还是个未知数。

今天我和你分享了迁移学习的基本原理和常用方法。其要点如下：

- 迁移学习是运用已学习的知识来求解不同但相关领域问题的新的机器学习方法，目的是让机器“学会学习”；
- 迁移学习适用于跨领域和小数据的学习任务；
- 迁移学习的任务类型可以分为归纳迁移学习，直推式迁移学习和无监督迁移学习；
- 迁移学习的学习方法包括基于样本、基于特征、基于模型和基于关系。

2016年，人工智能的大咖吴恩达表示“继深度学习之后，迁移学习将引领下一波机器学习技术”。那么你如何看待迁移学习的前景呢？

欢迎发表你的观点。

<img src="https://static001.geekbang.org/resource/image/a3/bb/a331dead77d7e3e1d9f1939ed38534bb.jpg" alt="">


